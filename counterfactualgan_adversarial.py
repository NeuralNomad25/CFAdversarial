# -*- coding: utf-8 -*-
"""CounterfactualGAN_adversarial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KGdq1sFJbo1S0JRs9_DftKugxHNmbkns

# Comparison of adversarial attacks with and without counterfactual adversarial attacks
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install adversarial-robustness-toolbox

"""## Data preprocessing"""

import pickle
import time
import warnings

from art.attacks.evasion import HopSkipJump, SignOPTAttack, ZooAttack
from art.estimators.classification import SklearnClassifier
import numpy as np
import pandas as pd
from sklearn.datasets import dump_svmlight_file
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
import sklearn.preprocessing
import xgboost as xgb

warnings.filterwarnings('ignore')

"""Download the dataset from Github to Google Colab and unzip it."""

!wget https://github.com/fisher85/ml-cybersecurity/blob/master/python-web-attack-detection/datasets/web_attacks_balanced.zip?raw=true -O dataset.zip
!unzip -u dataset.zip

"""Load our dataset. We use the balanced dataset based on CICIDS2017 (see the description of this balanced dataset in the previous work: https://ispranproceedings.elpub.ru/jour/article/view/1348/1147)."""

df = pd.read_csv('web_attacks_balanced.csv')
df

"""The dataset contains 4 classes:"""

df['Label'].unique()

"""Transform categorical labels into numeric form with simple label encoding."""

df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
df['Label'].unique()

"""Select the 10 most important features (see https://ispranproceedings.elpub.ru/jour/article/view/1348/1147)"""

webattack_features = ['Average Packet Size',
                      'Flow Bytes/s',
                      'Max Packet Length',
                      'Fwd IAT Min',
                      'Fwd Packet Length Mean',
                      'Total Length of Fwd Packets',
                      'Flow IAT Mean',
                      'Fwd IAT Std',
                      'Fwd Packet Length Max',
                      'Fwd Header Length']
df[webattack_features]

"""Get a target vector and a feature matrix of the training set."""

y = df['Label'].values
X = df[webattack_features].values

print(X.shape, y.shape)

"""Split the dataset into a training set and a test set."""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, shuffle=True, random_state=42)

X_train.shape

#scaler = sklearn.preprocessing.MinMaxScaler()

#dtrain = scaler.fit_transform(X_train)
#dtest = scaler.fit_transform(X_test)

#dump_svmlight_file(dtest, y_test, 'web_attacks_libsvm_normalized.test',
#                   zero_based=True, comment=None, query_id=None, multilabel=False)
#dump_svmlight_file(dtrain, y_train, 'web_attacks_libsvm_normalized.train',
#                   zero_based=True, comment=None, query_id=None, multilabel=False)

"""Download the archive with the already prepared data from Github to Google Colab and unzip it. This archive also contains saved models, instances of the attacks and all necessary data for the LT-Attack."""

!wget https://github.com/fisher85/ml-cybersecurity/blob/master/adversarial-attacks/data_for_comparison.zip?raw=true -O data_for_comparison.zip
!unzip -u data_for_comparison.zip

"""## Training a Random Forest model for ZOO, HSJA, SignOPT

We use a Scikit-learn version of a Random Forest classifier with ART implementations of ZOO, HSJA, and SignOPT attacks.
"""

model = RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=5, min_samples_split=2,
                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt',
                               max_leaf_nodes=50, min_impurity_decrease=0.0,
                               bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0,
                               warm_start=False, class_weight=None)
model.fit(X_train, y_train)

with open('model_for_comparison.sav', 'wb') as f:
   pickle.dump(model, f)

"""Load the previously trained model."""

model = pickle.load(open('model_for_comparison.sav', 'rb'))

"""Get the Random Forest model's evaluation metrics for the test data."""

y_pred = model.predict(X_test)
y_pred.shape

matrix = confusion_matrix(y_test, y_pred)
matrix

"""We use the following function to get evaluation metrics."""

def print_metrics(y_eval, y_pred, average='weighted'):
    accuracy = accuracy_score(y_eval, y_pred)
    precision = precision_score(y_eval, y_pred, average=average)
    recall = recall_score(y_eval, y_pred, average=average)
    f1 = f1_score(y_eval, y_pred, average=average)

    print('Accuracy =', accuracy)
    print('Precision =', precision)
    print('Recall =', recall)
    print('F1 =', f1)

print_metrics(y_test, y_pred)

"""Create an ART classifier for the trained model."""

art_classifier = SklearnClassifier(model=model)

"""## ZOO attack

Use a previously created attack instance for repeatability or uncomment and use the following code to create and save a ZOO attack instance.
"""

zoo = ZooAttack(classifier=art_classifier, confidence=10, targeted=False, learning_rate=1e-1, max_iter=10,
                binary_search_steps=20, initial_const=1e-3, abort_early=True, use_resize=False,
                use_importance=True, nb_parallel=10, batch_size=1, variable_h=0.5)

with open('zoo_for_comparison.sav', 'wb') as f:
   pickle.dump(zoo, f)

"""Load the previously created attack instance."""

zoo = pickle.load(open('zoo_for_comparison.sav', 'rb'))

"""Generate adversarial samples and measure elapsed time. This step may take some time, around 8 minutes or more depending on the hardware performance."""

start_time = time.time()
X_test_adv = zoo.generate(X_test)
print(f'Total time: {time.time() - start_time}')

"""Get predictions for the generated samples."""

y_pred_adv = model.predict(X_test_adv)
y_pred_adv.shape

def process_relevant_samples(labels, pred_labels, adv_labels):
    # Count all adversarial samples.
    print(f'Generated {np.count_nonzero(adv_labels != pred_labels)} adversarial samples'
          f' from {labels.shape[0]} original samples.')

    # Replace predictions for the irrelevant samples with original predictions for the test set.
    relevant_examples_mask = (adv_labels != pred_labels) & (
        pred_labels != 0) & (adv_labels == 0)
    adv_labels[~relevant_examples_mask] = pred_labels[~relevant_examples_mask]

    # Count only relevant adversarial samples.
    print(f'Generated {np.count_nonzero(adv_labels != pred_labels)} relevant adversarial samples'
          f' from {labels.shape[0]} original samples.')

    # Get model's evaluation metrics.
    matrix = confusion_matrix(labels, adv_labels)
    print('Confusion matrix:\n', matrix)
    print_metrics(labels, adv_labels)



process_relevant_samples(y_test, y_pred, y_pred_adv)

"""## Counterfactual GAN +ZOO"""

import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier

class CounterfactualGAN:
    def __init__(self, input_dim, classifier, target_label):
        """
        Initialize the Counterfactual GAN.

        Args:
            input_dim: Dimension of the input data.
            classifier: Pre-trained classifier (should support `predict_proba`).
            target_label: Target label for generating adversarial examples.
        """
        self.input_dim = input_dim
        self.latent_dim = 50
        self.target_label = target_label
        self.classifier = classifier
        self.generator = self.build_generator()
        self.discriminator = self.build_discriminator()
        self.gan = self.build_gan()

    def build_generator(self):
        """
        Build the generator model.
        Returns:
            Generator model.
        """
        model = models.Sequential([
            layers.InputLayer(input_shape=(self.latent_dim,)),
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(128, activation='relu'),
            layers.Dense(self.input_dim, activation='linear')  # Perturbations
        ])
        return model

    def build_discriminator(self):
        """
        Build the discriminator model.
        Returns:
            Discriminator model.
        """
        model = models.Sequential([
            layers.InputLayer(input_shape=(self.input_dim,)),
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(64, activation='relu'),
            layers.Dense(1, activation='sigmoid')
        ])
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model

    def build_gan(self):
        """
        Combine generator and discriminator into a GAN.
        Returns:
            GAN model.
        """
        self.discriminator.trainable = False
        input_latent = layers.Input(shape=(self.latent_dim,))
        generated = self.generator(input_latent)
        validity = self.discriminator(generated)
        model = models.Model(inputs=input_latent, outputs=validity)
        model.compile(optimizer='adam', loss='binary_crossentropy')
        return model

    def compute_target_loss(self, X, target_label):
        """
        Compute the target loss to guide the generator.
        Args:
            X: Input data.
            target_label: Target label for adversarial generation.
        Returns:
            Target loss value.
        """
        predictions = self.classifier.predict_proba(X)
        target_probs = tf.convert_to_tensor(predictions[:, target_label], dtype=tf.float32)
        return -tf.reduce_mean(tf.math.log(target_probs + 1e-10))  # Maximize probability of target label

    def train(self, X_train, epochs=500, batch_size=32, reconstruction_weight=10.0, target_loss_weight=10.0):
        """
        Train the Counterfactual GAN.

        Args:
            X_train: Training data (ensure it is np.float32).
            epochs: Number of training epochs.
            batch_size: Batch size.
            reconstruction_weight: Weight of the reconstruction loss.
            target_loss_weight: Weight of the targeted misclassification loss.
        """
        # Ensure the data type is float32
        X_train = X_train.astype(np.float32)

        # Adversarial labels
        valid = np.ones((batch_size, 1), dtype=np.float32)
        fake = np.zeros((batch_size, 1), dtype=np.float32)

        for epoch in range(epochs):
            # ---------------------
            # Train Discriminator
            # ---------------------
            idx = np.random.randint(0, X_train.shape[0], batch_size)
            real_data = X_train[idx]
            noise = np.random.normal(0, 1, (batch_size, self.latent_dim)).astype(np.float32)
            generated_data = self.generator.predict(noise)

            d_loss_real = self.discriminator.train_on_batch(real_data, valid)
            d_loss_fake = self.discriminator.train_on_batch(generated_data, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

            # ---------------------
            # Train Generator
            # ---------------------
            noise = np.random.normal(0, 1, (batch_size, self.latent_dim)).astype(np.float32)
            with tf.GradientTape() as tape:
                generated_data = self.generator(noise)
                validity = self.discriminator(generated_data)
                g_loss_adv = tf.reduce_mean(tf.math.log(validity + 1e-10))
                recon_loss = tf.reduce_mean(tf.abs(real_data - generated_data))
                target_loss = self.compute_target_loss(real_data, self.target_label)
                g_loss = g_loss_adv + reconstruction_weight * recon_loss + target_loss_weight * target_loss

            grads = tape.gradient(g_loss, self.generator.trainable_weights)
            self.gan.optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))

            # Print progress
            if epoch % 100 == 0:
                print(f"{epoch} [D loss: {d_loss[0]}, acc: {100 * d_loss[1]}] [G loss: {g_loss}]")

    def generate_noise(self, batch_size, shape):
        """
        Generates noise-based perturbations using the trained generator.
        Args:
            batch_size: Number of samples to generate.
            shape: Shape of the output perturbations.
        Returns:
            Generated perturbations.
        """
        noise = np.random.normal(0, 1, (batch_size, self.latent_dim)).astype(np.float32)
        perturbations = self.generator.predict(noise)
        return perturbations

# Main Code
# Step 1: Normalize your data
scaler = MinMaxScaler()
X_test_scaled = scaler.fit_transform(X_test).astype(np.float32)

# Step 2: Train your classifier (ensure it supports `predict_proba`)
classifier = RandomForestClassifier(n_estimators=100)
classifier.fit(X_train, y_train)

# Step 3: Initialize Counterfactual GAN
input_dim = X_test_scaled.shape[1]
target_label = 1  # Change this to the desired target class index
gan_model = CounterfactualGAN(input_dim=input_dim, classifier=classifier, target_label=target_label)

# Step 4: Train Counterfactual GAN
gan_model.train(X_test_scaled, epochs=500, batch_size=64, reconstruction_weight=10.0, target_loss_weight=10.0)

# Step 5: Generate Perturbations and Apply to Original Data
perturbations = gan_model.generate_noise(batch_size=len(X_test), shape=X_test_scaled.shape[1:])
X_test_perturbed = X_test_scaled + 0.1 * perturbations  # Add scaled perturbations
X_test_perturbed = scaler.inverse_transform(X_test_perturbed)  # Undo scaling

# Step 6: Generate adversarial examples using ZooAttack
start_time = time.time()
X_test_advzoo = zoo.generate(X_test_perturbed)
print(f'Total time: {time.time() - start_time}')

y_pred_advzoo = model.predict(X_test_advzoo)
y_pred_advzoo.shape

process_relevant_samples(y_test, y_pred, y_pred_advzoo)

"""## XAI for CFA-GAN

## without CFGAN
"""

!pip install lime

from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import numpy as np

attack_classes = ['BENIGN', 'XSS', 'Brute Force', 'Sql Injection']

explainer = LimeTabularExplainer(
    X_test,
    training_labels=y_test,
    mode="classification",
    class_names=attack_classes,
    discretize_continuous=True
)

i = 100
test_example = X_test[i].reshape(1, -1)

explanation = explainer.explain_instance(test_example[0], model.predict_proba)

explanation.show_in_notebook()

fig = explanation.as_pyplot_figure()
fig.suptitle("Feature Importance for Original Example", fontsize=16)
fig.tight_layout()
fig.subplots_adjust(top=0.85)

plt.savefig("feature_importance_original_example.png", dpi=300)
plt.close(fig)

print("Feature Importance for the Original Example:")
for feature, importance in explanation.as_list():
    print(f"{feature}: {importance:.4f}")

original_probs = model.predict_proba(test_example)[0]

print(f"\nPredicted probabilities for the original example (index {i}):")
for label, prob in zip(attack_classes, original_probs):
    print(f"{label}: {prob:.4f}")

fig, ax = plt.subplots(figsize=(10, 6))
bar_width = 0.35
index = np.arange(len(attack_classes))

ax.bar(index, original_probs, bar_width, label='Original', alpha=0.7)

ax.set_xlabel('Classes')
ax.set_ylabel('Predicted Probability')
ax.set_title('Predicted Probabilities for Original Example')
ax.set_xticks(index)
ax.set_xticklabels(attack_classes)
ax.legend()

plt.savefig("original_example_probabilities.png", dpi=300)
plt.close(fig)

print(f"\nPredicted probabilities for the original example (index {i}): {original_probs}")

class_index = attack_classes.index('XSS')

fig = explanation.as_pyplot_figure(label=class_index)
fig.suptitle(f"Feature Importance for {attack_classes[class_index]} Example", fontsize=16)
fig.tight_layout()
fig.subplots_adjust(top=0.85)

plt.savefig(f"feature_importance_{attack_classes[class_index]}.png", dpi=300)
plt.close(fig)

print(f"\nExplanation for the class '{attack_classes[class_index]}':")
for feature, importance in explanation.as_list(label=class_index):
    print(f"{feature}: {importance:.4f}")

df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
print("\nUnique labels in the dataset:", df['Label'].unique())

from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import numpy as np

webattack_features = [
    'Average Packet Size',
    'Flow Bytes/s',
    'Max Packet Length',
    'Fwd IAT Min',
    'Fwd Packet Length Mean',
    'Total Length of Fwd Packets',
    'Flow IAT Mean',
    'Fwd IAT Std',
    'Fwd Packet Length Max',
    'Fwd Header Length'
]

attack_classes = ['BENIGN', 'XSS', 'Brute Force', 'Sql Injection']

explainer = LimeTabularExplainer(
    X_test,
    training_labels=y_test,
    mode="classification",
    class_names=attack_classes,
    discretize_continuous=True
)

def explain_class(i, test_example):
    explanation = explainer.explain_instance(test_example[0], model.predict_proba)

    fig = explanation.as_pyplot_figure()
    fig.suptitle("Feature Importance for Original Example", fontsize=16)
    fig.tight_layout()
    fig.subplots_adjust(top=0.85)
    plt.savefig(f"feature_importance_original_example_{i}.png", dpi=300)
    plt.close(fig)

    print(f"Feature Importance for the Original Example (Index {i}):")
    for feature, importance in explanation.as_list():
        print(f"{feature}: {importance:.4f}")

    original_probs = model.predict_proba(test_example)[0]
    print(f"\nPredicted probabilities for the original example (Index {i}):")
    for label, prob in zip(attack_classes, original_probs):
        print(f"{label}: {prob:.4f}")

    fig, ax = plt.subplots(figsize=(10, 6))
    bar_width = 0.35
    index = np.arange(len(attack_classes))
    ax.bar(index, original_probs, bar_width, label='Original', alpha=0.7)
    ax.set_xlabel('Classes')
    ax.set_ylabel('Predicted Probability')
    ax.set_title('Predicted Probabilities for Original Example')
    ax.set_xticks(index)
    ax.set_xticklabels(attack_classes)
    ax.legend()
    plt.savefig(f"original_example_probabilities_{i}.png", dpi=300)
    plt.close(fig)

    for class_name in attack_classes:
        class_index = attack_classes.index(class_name)
        try:
            fig = explanation.as_pyplot_figure(label=class_index)
            fig.suptitle(f"Feature Importance for {class_name} Example", fontsize=16)
            fig.tight_layout()
            fig.subplots_adjust(top=0.85)
            plt.savefig(f"feature_importance_{class_name}_{i}.png", dpi=300)
            plt.close(fig)

            print(f"\nExplanation for the class '{class_name}' (Index {i}):")
            for feature, importance in explanation.local_exp[class_index]:
                feature_name = webattack_features[int(feature)]
                print(f"{feature_name}: {importance:.4f}")
        except KeyError:
            print(f"Error: Could not find explanation for class '{class_name}'.")

i = 100
test_example = X_test[i].reshape(1, -1)
explain_class(i, test_example)

df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
print("\nUnique labels in the dataset:", df['Label'].unique())

def explain_class(i, test_example):
    explanation = explainer.explain_instance(test_example[0], model.predict_proba)

    fig = explanation.as_pyplot_figure()
    fig.suptitle("Feature Importance for Original Example", fontsize=16)
    fig.tight_layout()
    fig.subplots_adjust(top=0.85)
    plt.savefig(f"feature_importance_original_example_{i}.png", dpi=300)
    plt.close(fig)

    print(f"Feature Importance for the Original Example (Index {i}):")
    for feature, importance in explanation.as_list():
        print(f"{feature}: {importance:.4f}")

    original_probs = model.predict_proba(test_example)[0]
    print(f"\nPredicted probabilities for the original example (Index {i}):")
    for label, prob in zip(attack_classes, original_probs):
        print(f"{label}: {prob:.4f}")

    fig, ax = plt.subplots(figsize=(10, 6))
    bar_width = 0.35
    index = np.arange(len(attack_classes))
    ax.bar(index, original_probs, bar_width, label='Original', alpha=0.7)
    ax.set_xlabel('Classes')
    ax.set_ylabel('Predicted Probability')
    ax.set_title('Predicted Probabilities for Original Example')
    ax.set_xticks(index)
    ax.set_xticklabels(attack_classes)
    ax.legend()
    plt.savefig(f"original_example_probabilities_{i}.png", dpi=300)
    plt.close(fig)

    for class_name in attack_classes:
        class_index = attack_classes.index(class_name)

        if class_index in explanation.local_exp:
            try:
                fig = explanation.as_pyplot_figure(label=class_index)
                fig.suptitle(f"Feature Importance for {class_name} Example", fontsize=16)
                fig.tight_layout()
                fig.subplots_adjust(top=0.85)
                plt.savefig(f"feature_importance_{class_name}_{i}.png", dpi=300)
                plt.close(fig)

                print(f"\nExplanation for the class '{class_name}' (Index {i}):")
                for feature, importance in explanation.local_exp[class_index]:
                    feature_name = webattack_features[int(feature)]
                    print(f"{feature_name}: {importance:.4f}")
            except KeyError:
                print(f"Error: Could not find explanation for class '{class_name}'.")
        else:
            print(f"Warning: Class index {class_index} does not exist in explanation for class '{class_name}'.")

i = 100
test_example = X_test[i].reshape(1, -1)
explain_class(i, test_example)

df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
print("\nUnique labels in the dataset:", df['Label'].unique())

import lime
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import numpy as np

# Assuming you have loaded X_train, y_train, X_test, X_test_adv, and your model already
# Define custom attack class names, if needed
attack_classes = ['BENIGN', 'XSS', 'Brute Force', 'Sql Injection']

# Initialize LIME explainer
explainer = LimeTabularExplainer(
    X_test,  # Training data for the model
    training_labels=y_test,  # Labels for the training data
    mode="classification",  # The task is classification
    class_names=attack_classes,  # Names of the classes (model's output)
    discretize_continuous=True  # This discretizes continuous features
)

# Choose an adversarial example to explain (e.g., index 5)
i = 100  # Index of adversarial example
adv_example = X_test[i].reshape(1, -1)  # Reshape to be a single example

# Generate explanation for the adversarial example
explanation = explainer.explain_instance(adv_example[0], model.predict_proba)

# Display explanation in notebook (interactive visualization)
explanation.show_in_notebook()

# Alternatively, you can plot a bar chart of feature importances
fig = explanation.as_pyplot_figure()
#fig.suptitle("Feature Importance for Original Example", fontsize=16)
fig.tight_layout()
fig.subplots_adjust(top=0.85)  # Adjust the top for the title to fit
fig.show()



# Checking unique labels in the DataFrame (just to validate the target labels)
df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
print(df['Label'].unique())

import lime
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import numpy as np

# Assuming you have loaded X_train, y_train, X_test, X_test_adv, and your model already
# Define custom attack class names, if needed
attack_classes = ['BENIGN', 'XSS', 'Brute Force', 'Sql Injection']

# Initialize LIME explainer
explainer = LimeTabularExplainer(
    X_test,  # Training data for the model
    training_labels=y_test,  # Labels for the training data
    mode="classification",  # The task is classification
    class_names=attack_classes,  # Names of the classes (model's output)
    discretize_continuous=True  # This discretizes continuous features
)

# Choose an example (e.g., index 100)
i = 100  # Index of the example to explain
adv_example = X_test[i].reshape(1, -1)  # Reshape to be a single example

# Generate explanation for the adversarial example
explanation = explainer.explain_instance(adv_example[0], model.predict_proba)

# Display explanation in notebook (interactive visualization)
explanation.show_in_notebook()

# Alternatively, you can plot a bar chart of feature importances
fig = explanation.as_pyplot_figure()
fig.suptitle("Feature Importance for Original Example", fontsize=16)
fig.tight_layout()
fig.subplots_adjust(top=0.85)  # Adjust the top for the title to fit
fig.show()

# Print the feature importances for the specific class (e.g., 'XSS' - class_index = 1)
class_index = 1  # For XSS
print(f"Feature importances for the class '{attack_classes[class_index]}':")
for feature, importance in explanation.local_exp[class_index]:
    # Feature name and importance score for each feature
    print(f"Feature: {feature}, Importance: {importance:.4f}")

# Checking unique labels in the DataFrame (just to validate the target labels)
df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
print("Unique labels in the DataFrame:", df['Label'].unique())

import lime
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import numpy as np

# Assuming you have loaded X_train, y_train, X_test, X_test_adv, and your model already
# Define custom attack class names, if needed
attack_classes = ['BENIGN', 'XSS', 'Brute Force', 'Sql Injection']

# Initialize LIME explainer
explainer = LimeTabularExplainer(
    X_test,  # Training data for the model
    training_labels=y_test,  # Labels for the training data
    mode="classification",  # The task is classification
    class_names=attack_classes,  # Names of the classes (model's output)
    discretize_continuous=True  # This discretizes continuous features
)

# Choose an example (e.g., index 100)
i = 100  # Index of the example to explain
adv_example = X_test[i].reshape(1, -1)  # Reshape to be a single example

# Generate explanation for the adversarial example
explanation = explainer.explain_instance(adv_example[0], model.predict_proba)

# Display explanation in notebook (interactive visualization)
explanation.show_in_notebook()

# Alternatively, you can plot a bar chart of feature importances
fig = explanation.as_pyplot_figure()
fig.suptitle("Feature Importance for Original Example", fontsize=16)
fig.tight_layout()
fig.subplots_adjust(top=0.85)  # Adjust the top for the title to fit
fig.show()

# Print the feature importances for the specific class (e.g., 'XSS' - class_index = 1)
class_index = 1  # For XSS
print(f"Feature importances for the class '{attack_classes[class_index]}':")
for feature, importance in explanation.local_exp[class_index]:
    # Feature name and importance score for each feature
    print(f"Feature: {feature}, Importance: {importance:.4f}")

# Get and print predicted probabilities for the original example and adversarial example
original_probs = model.predict_proba(X_test[i].reshape(1, -1))[0]  # Probabilities for the original example
adv_probs = model.predict_proba(adv_example)[0]  # Probabilities for the adversarial example

# Print probabilities for the original and adversarial example
print(f"\nPredicted probabilities for the original example (Index {i}):")
for class_name, prob in zip(attack_classes, original_probs):
    print(f"{class_name}: {prob:.4f}")

print(f"\nPredicted probabilities for the adversarial example (Index {i}):")
for class_name, prob in zip(attack_classes, adv_probs):
    print(f"{class_name}: {prob:.4f}")

# Checking unique labels in the DataFrame (just to validate the target labels)
df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
print("Unique labels in the DataFrame:", df['Label'].unique())

import lime
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import numpy as np

# Assuming you have loaded X_train, y_train, X_test, and your model already
# Define custom attack class names, if needed
attack_classes = ['BENIGN', 'XSS', 'Brute Force', 'Sql Injection']

# Initialize LIME explainer
explainer = LimeTabularExplainer(
    X_test,  # Training data for the model
    training_labels=y_test,  # Labels for the training data
    mode="classification",  # The task is classification
    class_names=attack_classes,  # Names of the classes (model's output)
    discretize_continuous=True  # This discretizes continuous features
)

# Choose an original example to explain (e.g., index 100)
i = 100  # Index of the example to explain
orig_example = X_test[i].reshape(1, -1)  # Reshape to be a single example

# Generate explanation for the original example
explanation = explainer.explain_instance(orig_example[0], model.predict_proba)

# Display explanation in notebook (interactive visualization)
# explanation.show_in_notebook()

# Alternatively, you can plot a bar chart of feature importances for the original example
fig = explanation.as_pyplot_figure()
#fig.suptitle("Feature Importance for Original Example", fontsize=16)
fig.tight_layout()
fig.subplots_adjust(top=0.85)  # Adjust the top for the title to fit

# Save the plot to a file
fig.savefig('feature_importance_original_example.png', dpi=300)  # Save as a high-quality PNG file
print("Feature importance plot saved as 'feature_importance_original_example.png'")

# Print the feature importances for the specific class (e.g., 'XSS' - class_index = 1)
class_index = 1  # For XSS (index 1)
print(f"Feature importances for the class '{attack_classes[class_index]}':")
for feature, importance in explanation.local_exp[class_index]:
    # Feature name and importance score for each feature
    print(f"Feature: {feature}, Importance: {importance:.4f}")

# Get and print predicted probabilities for the original example
original_probs = model.predict_proba(X_test[i].reshape(1, -1))[0]  # Probabilities for the original example

# Print probabilities for the original example
print(f"\nPredicted probabilities for the original example (Index {i}):")
for class_name, prob in zip(attack_classes, original_probs):
    print(f"{class_name}: {prob:.4f}")

# Save probabilities to a text file for documentation (optional)
with open("predicted_probabilities_original_example.txt", "w") as f:
    f.write(f"Predicted probabilities for the original example (Index {i}):\n")
    for class_name, prob in zip(attack_classes, original_probs):
        f.write(f"{class_name}: {prob:.4f}\n")

# Checking unique labels in the DataFrame (just to validate the target labels)
df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
print("Unique labels in the DataFrame:", df['Label'].unique())

import lime
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import numpy as np

# Assuming you have loaded X_train, y_train, X_test, and your model already
# Define custom attack class names, if needed
attack_classes = ['BENIGN', 'XSS', 'Brute Force', 'Sql Injection']

# Initialize LIME explainer
explainer = LimeTabularExplainer(
    X_test,  # Training data for the model
    training_labels=y_test,  # Labels for the training data
    mode="classification",  # The task is classification
    class_names=attack_classes,  # Names of the classes (model's output)
    discretize_continuous=True  # This discretizes continuous features
)

# Choose an original example to explain (e.g., index 100)
i = 156 #156  # Index of the example to explain
orig_example = X_test[i].reshape(1, -1)  # Reshape to be a single example

# Generate explanation for the original example
explanation = explainer.explain_instance(orig_example[0], model.predict_proba)

# Display explanation in notebook (interactive visualization)
# explanation.show_in_notebook()

# Alternatively, you can plot a bar chart of feature importances for the original example
fig = explanation.as_pyplot_figure()
#fig.suptitle("Feature Importance for Original Example", fontsize=16)
fig.tight_layout()
fig.subplots_adjust(top=0.85)  # Adjust the top for the title to fit

# Save the plot to a file
fig.savefig('1.feature_importance_original_examplexss_final.png', dpi=300)  # Save as a high-quality PNG file
print("Feature importance plot saved as 'feature_importance_original_example.png'")

# Print the feature importances for the specific class (e.g., 'XSS' - class_index = 1)
class_index = 1  # For XSS (index 1)
print(f"Feature importances for the class '{attack_classes[class_index]}':")
for feature, importance in explanation.local_exp[class_index]:
    # Feature name and importance score for each feature
    print(f"Feature: {feature}, Importance: {importance:.4f}")

# Get and print predicted probabilities for the original example
original_probs = model.predict_proba(X_test[i].reshape(1, -1))[0]  # Probabilities for the original example

# Print probabilities for the original example
print(f"\nPredicted probabilities for the original example (Index {i}):")
for class_name, prob in zip(attack_classes, original_probs):
    print(f"{class_name}: {prob:.4f}")

# Plot the class probabilities for the original example as a bar plot with hatching
fig, ax = plt.subplots(figsize=(6, 5))

bars = ax.bar(attack_classes, original_probs, color='skyblue', hatch='/', edgecolor='black')

# Add labels and title
ax.set_xlabel('Classes', fontsize=10)
ax.set_ylabel('Predicted Probability', fontsize=10)
#ax.set_title('Predicted Probabilities for the Original Example', fontsize=16)

# Add class names on the x-axis
ax.set_xticklabels(attack_classes, rotation=40, ha='right', fontsize=8)

# Add the probability values above each bar
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 4), ha='center', va='bottom', fontsize=8)

# Save the bar plot with hatching
plt.tight_layout()
plt.savefig('1.predicted_probabilities_original_example.png', dpi=300)  # Save as a high-quality PNG file
print("Predicted probabilityfinal_XSSorigninal.png'")

plt.show()

# Save probabilities to a text file for documentation (optional)
with open("predicted_probabilities_original_example.txt", "w") as f:
    f.write(f"Predicted probabilities for the original example (Index {i}):\n")
    for class_name, prob in zip(attack_classes, original_probs):
        f.write(f"{class_name}: {prob:.4f}\n")

# Checking unique labels in the DataFrame (just to validate the target labels)
df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
print("Unique labels in the DataFrame:", df['Label'].unique())

"""##Zoo Advarsarial Example"""

import lime
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import numpy as np


attack_classes = ['BENIGN', 'XSS', 'Brute Force', 'Sql Injection']

# Initialize LIME explainer
explainer = LimeTabularExplainer(
    X_test_adv,  # Training data for the model
    training_labels=y_test,  # Labels for the training data
    mode="classification",  # The task is classification
    class_names=attack_classes,  # Names of the classes (model's output)
    discretize_continuous=True  # This discretizes continuous features
)

# Choose an original example to explain (e.g., index 100)
i = 156 #156  # Index of the example to explain
orig_example = X_test_adv[i].reshape(1, -1)  # Reshape to be a single example

# Generate explanation for the original example
explanation = explainer.explain_instance(orig_example[0], model.predict_proba)

# Display explanation in notebook (interactive visualization)
# explanation.show_in_notebook()

# Alternatively, you can plot a bar chart of feature importances for the original example
fig = explanation.as_pyplot_figure()
#fig.suptitle("Feature Importance for Original Example", fontsize=16)
fig.tight_layout()
fig.subplots_adjust(top=0.85)  # Adjust the top for the title to fit

# Save the plot to a file
fig.savefig('2.feature_importance_examplexss_zoo.png', dpi=300)  # Save as a high-quality PNG file
print("Feature importance plot saved as 'feature_importance_original_example.png'")

# Print the feature importances for the specific class (e.g., 'XSS' - class_index = 1)
class_index = 1  # For XSS (index 1)
print(f"Feature importances for the class '{attack_classes[class_index]}':")
for feature, importance in explanation.local_exp[class_index]:
    # Feature name and importance score for each feature
    print(f"Feature: {feature}, Importance: {importance:.4f}")

# Get and print predicted probabilities for the original example
original_probs = model.predict_proba(X_test_adv[i].reshape(1, -1))[0]  # Probabilities for the original example

# Print probabilities for the original example
print(f"\nPredicted probabilities for the original example (Index {i}):")
for class_name, prob in zip(attack_classes, original_probs):
    print(f"{class_name}: {prob:.4f}")

# Plot the class probabilities for the original example as a bar plot with hatching
fig, ax = plt.subplots(figsize=(6, 5))

bars = ax.bar(attack_classes, original_probs, color='skyblue', hatch='/', edgecolor='black')

# Add labels and title
ax.set_xlabel('Classes', fontsize=10)
ax.set_ylabel('Predicted Probability', fontsize=10)
#ax.set_title('Predicted Probabilities for the Original Example', fontsize=16)

# Add class names on the x-axis
ax.set_xticklabels(attack_classes, rotation=40, ha='right', fontsize=8)

# Add the probability values above each bar
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 4), ha='center', va='bottom', fontsize=8)

# Save the bar plot with hatching
plt.tight_layout()
plt.savefig('2.predicted_probabilities_zoo.png', dpi=300)  # Save as a high-quality PNG file
print("Predicted probabilityfinal_XSSorigninal.png'")

plt.show()

# Save probabilities to a text file for documentation (optional)
with open("predicted_probabilities_original_example.txt", "w") as f:
    f.write(f"Predicted probabilities for the original example (Index {i}):\n")
    for class_name, prob in zip(attack_classes, original_probs):
        f.write(f"{class_name}: {prob:.4f}\n")

# Checking unique labels in the DataFrame (just to validate the target labels)
df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
print("Unique labels in the DataFrame:", df['Label'].unique())

"""###ZOO+CGAN"""

import time
import lime
from lime.lime_tabular import LimeTabularExplainer
import matplotlib.pyplot as plt
import numpy as np


attack_classes = ['BENIGN', 'XSS', 'Brute Force', 'Sql Injection']



# Predict labels for the adversarial examples
y_pred_advzoo = model.predict(X_test_advzoo)


# Initialize LIME explainer for adversarial examples
explainer = LimeTabularExplainer(
    X_test_advzoo,  # Adversarial test data
    training_labels=y_test,  # Original labels for the training data
    mode="classification",  # The task is classification
    class_names=attack_classes,  # Names of the classes (model's output)
    discretize_continuous=True  # This discretizes continuous features
)

# Choose an adversarial example to explain (e.g., index 156)
i = 156  # Index of the example to explain
adv_example = X_test_advzoo[i].reshape(1, -1)  # Reshape to be a single example

# Generate explanation for the adversarial example
explanation = explainer.explain_instance(adv_example[0], model.predict_proba)

# Display explanation in notebook (interactive visualization)
# explanation.show_in_notebook()

# Alternatively, plot a bar chart of feature importances for the adversarial example
fig = explanation.as_pyplot_figure()
#fig.suptitle("Feature Importance for Adversarial Example", fontsize=16)
fig.tight_layout()
fig.subplots_adjust(top=0.85)  # Adjust the top for the title to fit

# Save the plot to a file
fig.savefig('3.feature_importance_adversarial_exampleZOOCGAN.png', dpi=300)  # Save as a high-quality PNG file
print("Feature importance plot saved as 'feature_importance_adversarial_example.png'")

# Print the feature importances for the specific class (e.g., 'XSS' - class_index = 1)
class_index = 1  # For XSS (index 1)
print(f"Feature importances for the class '{attack_classes[class_index]}':")
for feature, importance in explanation.local_exp[class_index]:
    # Feature name and importance score for each feature
    print(f"Feature: {feature}, Importance: {importance:.4f}")

# Get and print predicted probabilities for the adversarial example
adv_probs = model.predict_proba(X_test_advzoo[i].reshape(1, -1))[0]  # Probabilities for the adversarial example

# Print probabilities for the adversarial example
print(f"\nPredicted probabilities for the adversarial example (Index {i}):")
for class_name, prob in zip(attack_classes, adv_probs):
    print(f"{class_name}: {prob:.4f}")

# Plot the class probabilities for the adversarial example as a bar plot with hatching
fig, ax = plt.subplots(figsize=(6, 5))

bars = ax.bar(attack_classes, adv_probs, color='skyblue', hatch='/', edgecolor='black')

# Add labels and title
ax.set_xlabel('Classes', fontsize=10)
ax.set_ylabel('Predicted Probability', fontsize=10)

# Add class names on the x-axis
ax.set_xticklabels(attack_classes, rotation=40, ha='right', fontsize=8)

# Add the probability values above each bar
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.01, round(yval, 4), ha='center', va='bottom', fontsize=8)

# Save the bar plot with hatching
plt.tight_layout()
plt.savefig('3.predicted_probabilities_ZOOCGANadversarial_example.png', dpi=300)  # Save as a high-quality PNG file
print("Predicted probability bar plot saved as 'predicted_probabilities_adversarial_example.png'")

plt.show()

# Save probabilities to a text file for documentation (optional)
with open("predicted_probabilities_adversarial_example.txt", "w") as f:
    f.write(f"Predicted probabilities for the adversarial example (Index {i}):\n")
    for class_name, prob in zip(attack_classes, adv_probs):
        f.write(f"{class_name}: {prob:.4f}\n")

# Checking unique labels in the DataFrame (just to validate the target labels)
df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN'
                                else 1 if x == 'Web Attack – XSS'
                                else 2 if x == 'Web Attack – Brute Force'
                                else 3 if x == 'Web Attack – Sql Injection'
                                else 4)
print("Unique labels in the DataFrame:", df['Label'].unique())

"""## HSJA attack

Use a previously created attack instance for repeatability or uncomment and use the following code to create and save a HopSkipJump attack instance.
"""

hsja = HopSkipJump(classifier=art_classifier, batch_size=64, targeted=False,
                  norm=2, max_iter=10, max_eval=10000,
                  init_eval=100, init_size=100, verbose=True)

with open('hsja_for_comparison.sav', 'wb') as f:
   pickle.dump(hsja, f)

"""Load the previously created attack instance."""

hsja = pickle.load(open('hsja_for_comparison.sav', 'rb'))

"""Generate adversarial samples and measure elapsed time.  This step may take some time, around 8 minutes or more depending on the hardware performance."""

start_time = time.time()
X_test_advhja = hsja.generate(X_test_perturbed)
print(f'Total time: {time.time() - start_time}')

"""Get predictions for the generated samples."""

y_pred_advhsja = model.predict(X_test_advhja)
y_pred_advhsja.shape

"""Evaluate the model's performance on the test data with relevant crafted adversarial samples."""

process_relevant_samples(y_test, y_pred, y_pred_advhsja)

"""## HSJA wihout Counterfactual GAN"""

start_time = time.time()
X_test_adv1 = hsja.generate(X_test)
print(f'Total time: {time.time() - start_time}')

y_pred_adv = model.predict(X_test_adv1)
y_pred_adv.shape

process_relevant_samples(y_test, y_pred, y_pred_adv)

"""## SignOPT attack

Use a previously created attack instance for repeatability or uncomment and use the following code to create and save a SignOPT attack instance.
"""

signopt = SignOPTAttack(estimator=art_classifier, targeted=False, epsilon=0.001,
                        num_trial=100, max_iter=10, query_limit=20000, k=200,
                        alpha=0.2, beta=0.001, eval_perform=False, batch_size=64, verbose=False)
signopt.clip_min = None
signopt.clip_max = None

with open('signopt_for_comparison.sav', 'wb') as f:
   pickle.dump(signopt, f)

"""Load the previously created attack instance."""

signopt = pickle.load(open('signopt_for_comparison.sav', 'rb'))

"""Generate adversarial samples and measure elapsed time.  This step may take some considerable time, around 1 hour or more depending on the hardware performance."""

start_time = time.time()
X_test_adv = signopt.generate(X_test)
print(f'Total time: {time.time() - start_time}')

"""Get predictions for the generated samples."""

y_pred_adv = model.predict(X_test_adv)
y_pred_adv.shape

"""Evaluate the model's performance on the test data with relevant crafted adversarial samples."""

process_relevant_samples(y_test, y_pred, y_pred_adv)

"""#CGAN+SigOPT"""

start_time = time.time()
X_test_advcgan = signopt.generate(X_test_perturbed)
print(f'Total time: {time.time() - start_time}')

y_pred_advsign = model.predict(X_test_advcgan)
y_pred_advsign.shape

process_relevant_samples(y_test, y_pred, y_pred_advsign)

"""## Conclusion

As we can see, the HSJA attack was the most effective attack: it needed less time to generate one relevant adversarial sample than any other attack. Similarly, ZOO was the least effective attack.

### Explainable AI CGAN+Zoo and ZOO
"""



"""## plotting of al results"""



import os
import zipfile
from google.colab import files

# Specify the directory and the extensions you want to download
directory = '/content/'
extensions = ['.png']  # Add more extensions as needed

# List all files in the directory
files_in_directory = os.listdir(directory)

# Filter files with the specified extensions
files_to_download = [file for file in files_in_directory if any(file.endswith(ext) for ext in extensions)]

# Define the name of the zip file
zip_filename = '/content/CFGANresults.zip'

# Create a zip file and add the matching files to it
with zipfile.ZipFile(zip_filename, 'w') as zipf:
    for file in files_to_download:
        file_path = os.path.join(directory, file)
        zipf.write(file_path, os.path.basename(file))  # Add file to zip

# Download the zip file
files.download(zip_filename)